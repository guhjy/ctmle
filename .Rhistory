gn_glmnets <- predict(gn_glmnets, newx = as.matrix(sapply(W, as.numeric)))
ctmle_glmnet_fit <- ctmleGeneral( Y = Y, A = A, W = W, Q = Q, ctmletype = 1,
gn_candidates = gn_glmnets,
gn_candidates_cv = gn_glmnets_cv,
folds = folds)
ctmle_glmnet_index <- ctmle_glmnet_fit$best_k
ctmle_glmnet_lambda <- glmnet_fit$lambda[glmnet_fit$lambda < glmnet_fit$lambda.min][ctmle_glmnet_fit$best_k]
SL.glmnet1 <- function(..., lambda = ctmle_glmnet_lambda){
SL.glmnet(..., lambda = lambda)
}
SL.library <- c(xgb_best_ctmle, 'SL.glmnet1')
SL_final <- SuperLearner(Y = A, X = W, SL.library = SL.library, cvControl = list(V = 5L), verbose = TRUE)
tauhatnaive <- mean(Y1)-mean(Y0)
tauhatnaive
tauhatimp <- mean(beta1hat)-mean(beta0hat)
tauhatimp
tauhatiptw <- mean(A * Y /(g1W) - (1-A)*Y/(1- g1W))
tauhatiptw
tauhatdr <- mean(((A*Y)-(A-g1W) * beta1hat)/g1W)-
mean((((1-A)*Y)+(A-g1W)*beta0hat)/(1-g1W))
tauhatdr
DR_SE <- sqrt(var((((A*Y)-(A-g1W) * beta1hat)/g1W)-
((((1-A)*Y)+(A-g1W)*beta0hat)/(1-g1W)))/n)
DR_ci <-  tauhatdr + DR_SE  * c(-1.96, 1.96)
tauhattmlemod <- tmle(Y=Y,A=A,W=data.frame(W=W),Q=Q,g1W=g1W)
tauhattmle <- tauhattmlemod$estimates$ATE$psi
dtrain <- xgb.DMatrix(as.matrix(sapply(W, as.numeric)), label = A)
cv <- xgb.cv(dtrain, nrounds = 10000, max_depth=4, eta=0.1, minchildweight = 10,  objective = "binary:logistic")
library(stringr)
library(xtable)
library(MASS)
library(dplyr)
library(tmle)
library(doParallel)
library(ggplot2)
library(reshape2)
# setwd('~/Desktop/CTMLE-continue/SL_CTMLE0/')
library(ctmle)
source('xgb_try.R')
n <- 1000
partial <- 5
Rep = 100
registerDoParallel(20)
name <- 'noac'
gamma <- 0.025
gnCV = FALSE
data <- read.csv(paste('data/screened_', name,  '.csv', sep = ''))
A_full <- data[,1]
# 1 to 20 are baseline
W_full <- as.matrix(data[,3:dim(data)[2]])
if(name == 'noac'){
W_full <- W_full[,-c(49,50)]
}
set.seed(1992)
confounder_ind <- order(cor(A_full, W_full))[1:40]
confounder_ind  <- confounder_ind[order(confounder_ind)]
Qbeta <- -abs(rnorm(length(confounder_ind))) * 0.38
Y_0 <- 2 + W_full[, confounder_ind ] %*% as.matrix(Qbeta) + 1 * 0
Y_1 <- 2 + W_full[, confounder_ind ] %*% as.matrix(Qbeta) + 1 * 1
Y_full <- 10 + W_full[, confounder_ind ] %*% as.matrix(Qbeta) + 1 * A_full
psi_true <- mean(Y_1 - Y_0)
psi_true
psi_naive <- mean(Y_full[A_full == 1]) - mean(Y_full[A_full == 0])
psi_naive
hist(Y_full)
N <- length(Y_full)
ind <- sample(1:N, n, replace = FALSE)
Y <- Y_full[ind]
A <- A_full[ind]
W <- data.frame(W_full[ind, ])
W_base <- W[, 1:partial]
Y1<-Y[which(A==1)]
Y0<-Y[which(A==0)]
treated <- data.frame(cbind(Y1, W[which(A==1),confounder_ind[1:partial]]))
untreated <- data.frame(cbind(Y0, W[which(A==0),confounder_ind[1:partial]]))
beta1hat<-predict(glm(Y1 ~., data = treated), newdata= W[,confounder_ind[1:partial]])
beta0hat<-predict(glm(Y0 ~., data = untreated), newdata= W[,confounder_ind[1:partial]])
Q <-matrix(c(beta0hat,beta1hat), ncol=2)
V = 5
folds <-by(sample(1:n, n), rep(1:V, length = n), list)
SL.library = c('SL.glmnet', 'SL.xgboost1')
gn_base_fit = ctmle::build_gn_seq(A, W, SL.library , folds = folds)
weights = gn_base_fit$details$coef
if(gnCV){
gn_base = gn_base_fit$gn_candidates
}else{
gn_base = gn_base_fit$gn_candidates_cv
}
g1W <-  bound(gn_base %*% weights, c(gamma, 1- gamma))
tauhatnaive <- mean(Y1)-mean(Y0)
tauhatnaive
tauhatnaive <- mean(Y1)-mean(Y0)
tauhatnaive
tauhatimp <- mean(beta1hat)-mean(beta0hat)
tauhatimp
tauhatiptw <- mean(A * Y /(g1W) - (1-A)*Y/(1- g1W))
tauhatiptw
tauhatdr <- mean(((A*Y)-(A-g1W) * beta1hat)/g1W)-
mean((((1-A)*Y)+(A-g1W)*beta0hat)/(1-g1W))
tauhatdr
DR_SE <- sqrt(var((((A*Y)-(A-g1W) * beta1hat)/g1W)-
((((1-A)*Y)+(A-g1W)*beta0hat)/(1-g1W)))/n)
DR_ci <-  tauhatdr + DR_SE  * c(-1.96, 1.96)
tauhattmlemod <- tmle(Y=Y,A=A,W=data.frame(W=W),Q=Q,g1W=g1W)
tauhattmle <- tauhattmlemod$estimates$ATE$psi
dtrain <- xgb.DMatrix(as.matrix(sapply(W, as.numeric)), label = A)
cv <- xgb.cv(dtrain, nrounds = 10000, max_depth=4, eta=0.1, minchildweight = 10,  objective = "binary:logistic")
cv <- xgb.cv(data = dtrain, nrounds = 10000, nthread = 1, nfold = 5, minchildweight = 10,
metrics = list("logloss"), early_stopping_rounds = 50,
max_depth = 4, eta = 0.1, objective = "binary:logistic")
xgb_best <- cv$best_iteration
xgbs <- create.SL.xgboost(tune = list(ntrees = xgb_best + (1:20), max_depth = c(4), shrinkage = c(0.1),
minobspernode = c(10)))
gn_xgbs = ctmle::build_gn_seq(A, W, xgbs$names , folds = folds)
ctmle_xgb_fit <- ctmleGeneral( Y = Y, A = A, W = W, Q = Q, ctmletype = 1,
gn_candidates = gn_xgbs$gn_candidates,
gn_candidates_cv = gn_xgbs$gn_candidates_cv,
folds = folds)
ctmle_xgb_index <- ctmle_xgb_fit$best_k
xgb_best_ctmle <- xgbs$names[ctmle_index]
xgb_best_ctmle
xgb_best_ctmle <- xgbs$names[ctmle_xgb_index]
xgb_best_ctmle
foldid <- rep(0, n)
for(i in 1:length(folds)){
foldid[folds[[i]]] = i
}
glmnet_fit <- cv.glmnet(x = as.matrix(sapply(W, as.numeric)), y = A, foldid = foldid, family = 'binomial', keep = TRUE)
gn_glmnets_cv <- glmnet_fit$fit.preval[, glmnet_fit$lambda < glmnet_fit$lambda.min]
gn_glmnets_cv <- gn_glmnets_cv[,colSums(!is.na(gn_glmnets_cv)) > 0]
lambdas <- glmnet_fit$lambda[glmnet_fit$lambda < glmnet_fit$lambda.min]
gn_glmnets <- glmnet(y = A, x = as.matrix(sapply(W, as.numeric)), lambda = lambdas)
gn_glmnets <- predict(gn_glmnets, newx = as.matrix(sapply(W, as.numeric)))
ctmle_glmnet_fit <- ctmleGeneral( Y = Y, A = A, W = W, Q = Q, ctmletype = 1,
gn_candidates = gn_glmnets,
gn_candidates_cv = gn_glmnets_cv,
folds = folds)
ctmle_glmnet_index <- ctmle_glmnet_fit$best_k
ctmle_glmnet_lambda <- glmnet_fit$lambda[glmnet_fit$lambda < glmnet_fit$lambda.min][ctmle_glmnet_fit$best_k]
SL.glmnet1 <- function(..., lambda = ctmle_glmnet_lambda){
SL.glmnet(..., lambda = lambda)
}
SL.library <- c(xgb_best_ctmle, 'SL.glmnet1')
SL_final <- SuperLearner(Y = A, X = W, SL.library = SL.library, cvControl = list(V = 5L), verbose = TRUE)
gn_SL <- predict(SL_final)$pred
tauhatctmlemod <- tmle(Y=Y,A=A,W=data.frame(W=W),Q=Q,g1W=gn_SL)
tauhatctmle <- tauhatctmlemod$estimates$ATE$psi
tauhats <- data.frame(Naive=tauhatnaive,
Imp=tauhatimp,
IPTW = tauhatiptw,
DR=tauhatdr,
TMLE=tauhattmle,
CTMLE0=tauhatctmle
)
tauhats
ind <- sample(1:N, n, replace = FALSE)
Y <- Y_full[ind]
A <- A_full[ind]
W <- data.frame(W_full[ind, ])
W_base <- W[, 1:partial]
Y1<-Y[which(A==1)]
Y0<-Y[which(A==0)]
treated <- data.frame(cbind(Y1, W[which(A==1),confounder_ind[1:partial]]))
untreated <- data.frame(cbind(Y0, W[which(A==0),confounder_ind[1:partial]]))
#Initial Q-estimate
beta1hat<-predict(glm(Y1 ~., data = treated), newdata= W[,confounder_ind[1:partial]])
beta0hat<-predict(glm(Y0 ~., data = untreated), newdata= W[,confounder_ind[1:partial]])
Q <-matrix(c(beta0hat,beta1hat), ncol=2)
############################################################
############################################################
V = 5
folds <-by(sample(1:n, n), rep(1:V, length = n), list)
SL.library = c('SL.glmnet', 'SL.xgboost1')
gn_base_fit = ctmle::build_gn_seq(A, W, SL.library , folds = folds)
weights = gn_base_fit$details$coef
if(gnCV){
gn_base = gn_base_fit$gn_candidates
}else{
gn_base = gn_base_fit$gn_candidates_cv
}
g1W <-  bound(gn_base %*% weights, c(gamma, 1- gamma))
#----------------------------------------
#Initial g-estimate
# Naive estimator
tauhatnaive <- mean(Y1)-mean(Y0)
tauhatnaive
# MLE
tauhatimp <- mean(beta1hat)-mean(beta0hat)
tauhatimp
# IPTW
tauhatiptw <- mean(A * Y /(g1W) - (1-A)*Y/(1- g1W))
tauhatiptw
# A-ITPW
tauhatdr <- mean(((A*Y)-(A-g1W) * beta1hat)/g1W)-
mean((((1-A)*Y)+(A-g1W)*beta0hat)/(1-g1W))
tauhatdr
#!!! SE for A-IPTW
DR_SE <- sqrt(var((((A*Y)-(A-g1W) * beta1hat)/g1W)-
((((1-A)*Y)+(A-g1W)*beta0hat)/(1-g1W)))/n)
DR_ci <-  tauhatdr + DR_SE  * c(-1.96, 1.96)
# TMLE
tauhattmlemod <- tmle(Y=Y,A=A,W=data.frame(W=W),Q=Q,g1W=g1W)
tauhattmle <- tauhattmlemod$estimates$ATE$psi
################################################################
################################################################
# 1. Use C-TMLE to tune seperately
# 2. Use SL to combine
# 3. Plug in for TMLE
# First for XGB
dtrain <- xgb.DMatrix(as.matrix(sapply(W, as.numeric)), label = A)
cv <- xgb.cv(data = dtrain, nrounds = 10000, nthread = 1, nfold = 5, minchildweight = 10,
metrics = list("logloss"), early_stopping_rounds = 50,
max_depth = 4, eta = 0.1, objective = "binary:logistic")
xgb_best <- cv$best_iteration
xgbs <- create.SL.xgboost(tune = list(ntrees = xgb_best + (1:20), max_depth = c(4), shrinkage = c(0.1),
minobspernode = c(10)))
gn_xgbs = ctmle::build_gn_seq(A, W, xgbs$names , folds = folds)
ctmle_xgb_fit <- ctmleGeneral( Y = Y, A = A, W = W, Q = Q, ctmletype = 1,
gn_candidates = gn_xgbs$gn_candidates,
gn_candidates_cv = gn_xgbs$gn_candidates_cv,
folds = folds)
ctmle_xgb_index <- ctmle_xgb_fit$best_k
xgb_best_ctmle <- xgbs$names[ctmle_xgb_index]
# Glmnet
foldid <- rep(0, n)
for(i in 1:length(folds)){
foldid[folds[[i]]] = i
}
glmnet_fit <- cv.glmnet(x = as.matrix(sapply(W, as.numeric)), y = A, foldid = foldid, family = 'binomial', keep = TRUE)
gn_glmnets_cv <- glmnet_fit$fit.preval[, glmnet_fit$lambda < glmnet_fit$lambda.min]
gn_glmnets_cv <- gn_glmnets_cv[,colSums(!is.na(gn_glmnets_cv)) > 0]
lambdas <- glmnet_fit$lambda[glmnet_fit$lambda < glmnet_fit$lambda.min]
gn_glmnets <- glmnet(y = A, x = as.matrix(sapply(W, as.numeric)), lambda = lambdas)
gn_glmnets <- predict(gn_glmnets, newx = as.matrix(sapply(W, as.numeric)))
ctmle_glmnet_fit <- ctmleGeneral( Y = Y, A = A, W = W, Q = Q, ctmletype = 1,
gn_candidates = gn_glmnets,
gn_candidates_cv = gn_glmnets_cv,
folds = folds)
ctmle_glmnet_index <- ctmle_glmnet_fit$best_k
ctmle_glmnet_lambda <- glmnet_fit$lambda[glmnet_fit$lambda < glmnet_fit$lambda.min][ctmle_glmnet_fit$best_k]
SL.glmnet1 <- function(..., lambda = ctmle_glmnet_lambda){
SL.glmnet(..., lambda = lambda)
}
SL.library <- c(xgb_best_ctmle, 'SL.glmnet1')
SL_final <- SuperLearner(Y = A, X = W, SL.library = SL.library, cvControl = list(V = 5L), verbose = TRUE)
gn_SL <- predict(SL_final)$pred
tauhatctmlemod <- tmle(Y=Y,A=A,W=data.frame(W=W),Q=Q,g1W=gn_SL)
tauhatctmle <- tauhatctmlemod$estimates$ATE$psi
################################################################
################################################################
# Return values
tauhats <- data.frame(Naive=tauhatnaive,
Imp=tauhatimp,
IPTW = tauhatiptw,
DR=tauhatdr,
TMLE=tauhattmle,
CTMLE0=tauhatctmle
)
tauhats
gn_SL[gn_SL > 0.975] = 0.975
gn_SL[gn_SL > 0.025] = 0.025
tauhatiptw_c <- mean(A * Y /(gn_SL) - (1-A)*Y/(1- gn_SL))
tauhatiptw_c
gn_SL
gn_SL <- predict(SL_final)$pred
gn_SL[gn_SL > 0.975] = 0.975
gn_SL[gn_SL < 0.025] = 0.025
tauhatiptw_c <- mean(A * Y /(gn_SL) - (1-A)*Y/(1- gn_SL))
tauhatiptw_c
gn_SL
gn_SL
tauhatiptw_c <- mean(A * Y /(gn_SL) - (1-A)*Y/(1- gn_SL))
tauhatiptw_c
tauhatiptw_c <- mean(A * Y /(gn_SL) - (1-A)*Y/(1- gn_SL))
tauhatiptw_c
tauhatdr_c <- mean(((A*Y)-(A-gn_SL) * beta1hat)/gn_SL)-
mean((((1-A)*Y)+(A-gn_SL)*beta0hat)/(1-gn_SL))
tauhatdr
tauhatiptw
tauhatiptw_c
gn_SL
DR_SE_c <- sqrt(var((((A*Y)-(A-gn_SL) * beta1hat)/gn_SL)-
((((1-A)*Y)+(A-gn_SL)*beta0hat)/(1-gn_SL)))/n)
DR_ci_c <-  tauhatdr + DR_SE  * c(-1.96, 1.96)
DR_SE_c
DR_ci_c <-  tauhatdr_c + DR_SE_c  * c(-1.96, 1.96)
DR_ci_c
tauhatdr
tauhatdr_c
DR_SE_c <- sqrt(var((((A*Y)-(A-gn_SL) * beta1hat)/gn_SL)-
((((1-A)*Y)+(A-gn_SL)*beta0hat)/(1-gn_SL)))/n)
DR_ci_c <-  tauhatdr_c + DR_SE_c  * c(-1.96, 1.96)
tauhatctmlemod <- tmle(Y=Y,A=A,W=data.frame(W=W),Q=Q,g1W=gn_SL)
tauhatctmle <- tauhatctmlemod$estimates$ATE$psi
tauhatctmle
tauhattmle_c <- tauhattmlemod_c$estimates$ATE$psi
tauhattmlemod_c <- tmle(Y=Y,A=A,W=data.frame(W=W),Q=Q,g1W=gn_SL)
tauhattmle_c <- tauhattmlemod_c$estimates$ATE$psi
tauhattmle_c
tauhats <- data.frame(Naive=tauhatnaive,
Imp=tauhatimp,
IPTW = tauhatiptw,
IPTW_c = tauhatiptw_c,
DR=tauhatdr,
DR_c = tauhatdr_c,
TMLE = tauhattmle,
TMLE_c = tauhattmle_c
)
tauhats
tauhattmlemod_c <- tmle(Y=Y,A=A,W=data.frame(W=W),Q=Q, g1W=gn_SL)
tauhattmle_c <- tauhattmlemod_c$estimates$ATE$psi
tauhats <- data.frame(Naive=tauhatnaive,
Imp=tauhatimp,
IPTW = tauhatiptw,
IPTW_c = tauhatiptw_c,
DR=tauhatdr,
DR_c = tauhatdr_c,
TMLE = tauhattmle,
TMLE_c = tauhattmle_c
)
tauhats
covered(DR_ci)
covered <- function(x){(x[1] < 1) & (x[2] > 1)}
ci_len  <- function(x){(x[2] - x[1])}
ci <- data.frame(
DR = covered(DR_ci),
DR_c = covered(DR_ci_c),
TMLE = covered(tauhattmlemod$estimates$ATE$CI),
TMLE_c = covered(tauhattmlemod_c$estimates$ATE$CI)
)
DR_ci
library(stringr)
library(xtable)
library(MASS)
library(dplyr)
library(tmle)
library(doParallel)
library(ggplot2)
library(reshape2)
# setwd('~/Desktop/CTMLE-continue/SL_CTMLE0/')
library(ctmle)
source('xgb_try.R')
n <- 1000
partial <- 5
Rep = 100
registerDoParallel(20)
name <- 'noac'
gamma <- 0.025
gnCV = FALSE
data <- read.csv(paste('data/screened_', name,  '.csv', sep = ''))
A_full <- data[,1]
# 1 to 20 are baseline
W_full <- as.matrix(data[,3:dim(data)[2]])
if(name == 'noac'){
W_full <- W_full[,-c(49,50)]
}
set.seed(1992)
confounder_ind <- order(cor(A_full, W_full))[1:40]
confounder_ind  <- confounder_ind[order(confounder_ind)]
Qbeta <- -abs(rnorm(length(confounder_ind))) * 0.38
Y_0 <- 2 + W_full[, confounder_ind ] %*% as.matrix(Qbeta) + 1 * 0
Y_1 <- 2 + W_full[, confounder_ind ] %*% as.matrix(Qbeta) + 1 * 1
Y_full <- 10 + W_full[, confounder_ind ] %*% as.matrix(Qbeta) + 1 * A_full
psi_true <- mean(Y_1 - Y_0)
psi_true
psi_naive <- mean(Y_full[A_full == 1]) - mean(Y_full[A_full == 0])
psi_naive
hist(Y_full)
N <- length(Y_full)
print(i)
ind <- sample(1:N, n, replace = FALSE)
Y <- Y_full[ind]
A <- A_full[ind]
W <- data.frame(W_full[ind, ])
W_base <- W[, 1:partial]
Y1<-Y[which(A==1)]
Y0<-Y[which(A==0)]
treated <- data.frame(cbind(Y1, W[which(A==1),confounder_ind[1:partial]]))
untreated <- data.frame(cbind(Y0, W[which(A==0),confounder_ind[1:partial]]))
#Initial Q-estimate
beta1hat<-predict(glm(Y1 ~., data = treated), newdata= W[,confounder_ind[1:partial]])
beta0hat<-predict(glm(Y0 ~., data = untreated), newdata= W[,confounder_ind[1:partial]])
Q <-matrix(c(beta0hat,beta1hat), ncol=2)
############################################################
############################################################
V = 5
folds <-by(sample(1:n, n), rep(1:V, length = n), list)
SL.library = c('SL.glmnet', 'SL.xgboost1')
gn_base_fit = ctmle::build_gn_seq(A, W, SL.library , folds = folds)
weights = gn_base_fit$details$coef
if(gnCV){
gn_base = gn_base_fit$gn_candidates
}else{
gn_base = gn_base_fit$gn_candidates_cv
}
g1W <-  bound(gn_base %*% weights, c(gamma, 1- gamma))
#----------------------------------------
#Initial g-estimate
# Naive estimator
tauhatnaive <- mean(Y1)-mean(Y0)
tauhatnaive
# MLE
tauhatimp <- mean(beta1hat)-mean(beta0hat)
tauhatimp
# IPTW
tauhatiptw <- mean(A * Y /(g1W) - (1-A)*Y/(1- g1W))
tauhatiptw
# A-ITPW
tauhatdr <- mean(((A*Y)-(A-g1W) * beta1hat)/g1W)-
mean((((1-A)*Y)+(A-g1W)*beta0hat)/(1-g1W))
tauhatdr
#!!! SE for A-IPTW
DR_SE <- sqrt(var((((A*Y)-(A-g1W) * beta1hat)/g1W)-
((((1-A)*Y)+(A-g1W)*beta0hat)/(1-g1W)))/n)
DR_ci <-  tauhatdr + DR_SE  * c(-1.96, 1.96)
# TMLE
tauhattmlemod <- tmle(Y=Y,A=A,W=data.frame(W=W),Q=Q,g1W=g1W)
tauhattmle <- tauhattmlemod$estimates$ATE$psi
################################################################
################################################################
# 1. Use C-TMLE to tune seperately
# 2. Use SL to combine
# 3. Plug in for TMLE
# First for XGB
dtrain <- xgb.DMatrix(as.matrix(sapply(W, as.numeric)), label = A)
cv <- xgb.cv(data = dtrain, nrounds = 10000, nthread = 1, nfold = 5, minchildweight = 10,
metrics = list("logloss"), early_stopping_rounds = 50,
max_depth = 4, eta = 0.1, objective = "binary:logistic")
xgb_best <- cv$best_iteration
xgbs <- create.SL.xgboost(tune = list(ntrees = xgb_best + (1:20), max_depth = c(4), shrinkage = c(0.1),
minobspernode = c(10)))
gn_xgbs = ctmle::build_gn_seq(A, W, xgbs$names , folds = folds)
ctmle_xgb_fit <- ctmleGeneral( Y = Y, A = A, W = W, Q = Q, ctmletype = 1,
gn_candidates = gn_xgbs$gn_candidates,
gn_candidates_cv = gn_xgbs$gn_candidates_cv,
folds = folds)
ctmle_xgb_index <- ctmle_xgb_fit$best_k
xgb_best_ctmle <- xgbs$names[ctmle_xgb_index]
# Glmnet
foldid <- rep(0, n)
for(i in 1:length(folds)){
foldid[folds[[i]]] = i
}
glmnet_fit <- cv.glmnet(x = as.matrix(sapply(W, as.numeric)), y = A, foldid = foldid, family = 'binomial', keep = TRUE)
gn_glmnets_cv <- glmnet_fit$fit.preval[, glmnet_fit$lambda < glmnet_fit$lambda.min]
gn_glmnets_cv <- gn_glmnets_cv[,colSums(!is.na(gn_glmnets_cv)) > 0]
lambdas <- glmnet_fit$lambda[glmnet_fit$lambda < glmnet_fit$lambda.min]
gn_glmnets <- glmnet(y = A, x = as.matrix(sapply(W, as.numeric)), lambda = lambdas)
gn_glmnets <- predict(gn_glmnets, newx = as.matrix(sapply(W, as.numeric)))
ctmle_glmnet_fit <- ctmleGeneral( Y = Y, A = A, W = W, Q = Q, ctmletype = 1,
gn_candidates = gn_glmnets,
gn_candidates_cv = gn_glmnets_cv,
folds = folds)
ctmle_glmnet_index <- ctmle_glmnet_fit$best_k
ctmle_glmnet_lambda <- glmnet_fit$lambda[glmnet_fit$lambda < glmnet_fit$lambda.min][ctmle_glmnet_fit$best_k]
SL.glmnet1 <- function(..., lambda = ctmle_glmnet_lambda){
SL.glmnet(..., lambda = lambda)
}
SL.library <- c(xgb_best_ctmle, 'SL.glmnet1')
SL_final <- SuperLearner(Y = A, X = W, SL.library = SL.library, cvControl = list(V = 5L), verbose = TRUE)
gn_SL <- predict(SL_final)$pred
######
gn_SL[gn_SL > 0.975] = 0.975
gn_SL[gn_SL < 0.025] = 0.025
tauhatiptw_c <- mean(A * Y /(gn_SL) - (1-A)*Y/(1- gn_SL))
tauhatiptw_c
# A-ITPW
tauhatdr_c <- mean(((A*Y)-(A-gn_SL) * beta1hat)/gn_SL)-
mean((((1-A)*Y)+(A-gn_SL)*beta0hat)/(1-gn_SL))
tauhatdr_c
#!!! SE for A-IPTW
DR_SE_c <- sqrt(var((((A*Y)-(A-gn_SL) * beta1hat)/gn_SL)-
((((1-A)*Y)+(A-gn_SL)*beta0hat)/(1-gn_SL)))/n)
DR_ci_c <-  tauhatdr_c + DR_SE_c  * c(-1.96, 1.96)
tauhattmlemod_c <- tmle(Y=Y,A=A,W=data.frame(W=W),Q=Q, g1W=gn_SL)
tauhattmle_c <- tauhattmlemod_c$estimates$ATE$psi
tauhats <- data.frame(Naive=tauhatnaive,
Imp=tauhatimp,
IPTW = tauhatiptw,
IPTW_c = tauhatiptw_c,
DR=tauhatdr,
DR_c = tauhatdr_c,
TMLE = tauhattmle,
TMLE_c = tauhattmle_c
)
tauhats
gn_base_fit$details
gn_base_fit$details
SL_final
covered <- function(x){(x[1] < 1) & (x[2] > 1)}
ci_len  <- function(x){(x[2] - x[1])}
ci <- data.frame(
DR = covered(DR_ci),
DR_c = covered(DR_ci_c),
TMLE = covered(tauhattmlemod$estimates$ATE$CI),
TMLE_c = covered(tauhattmlemod_c$estimates$ATE$CI)
)
ci
ci_length <- data.frame(
DR = ci_len(DR_ci),
DR_c = ci_len(DR_ci_c),
TMLE = ci_len(tauhattmlemod$estimates$ATE$CI),
TMLE_c = ci_len(tauhattmlemod_c$estimates$ATE$CI)
)
ci_length
tauhattmlemod_c$estimates$ATE$CI
tauhattmlemod$estimates$ATE$CI
DR_ci
DR_ci_c
library(devtools)
check("ctmle", cran = TRUE)
check("ctmle", cran = TRUE)
